{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "notebookPath": "kws-homework/kws.ipynb",
    "accelerator": "GPU",
    "language_info": {
      "version": "3.7.7",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py"
    },
    "notebookId": "3e35d411-455e-4fc7-abbb-fe61cfdaed88",
    "colab": {
      "name": "seminar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Yandex DataSphere Kernel",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "5e2enn8gcfcdcgemhdv7tw",
        "id": "_lhrn5O-qUYZ"
      },
      "source": [
        "# Import and misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDYpjMsOoHy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e562d4-de7b-427b-d4f1-b9ffc2f9af24"
      },
      "source": [
        "!git clone https://github.com/kostyayatsok/kws-homework.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kws-homework'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 9 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me4LwANP63Ji"
      },
      "source": [
        "import os\n",
        "os.chdir('./kws-homework/')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meO-Mp9jiAFC",
        "cellId": "yi282fxdfp9a88i1dzv9pi",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b31ba6-6fd4-4fd1-c9e8-15025a878f6d"
      },
      "source": [
        "%pip install torchaudio==0.9.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==0.9.1\n",
            "  Downloading torchaudio-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 4.2 MB/s \n",
            "\u001b[?25hCollecting torch==1.9.1\n",
            "  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 1.4 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.1->torchaudio==0.9.1) (3.10.0.2)\n",
            "Installing collected packages: torch, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.9.1 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.9.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.1 torchaudio-0.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbUpoArCqUYa",
        "cellId": "2mdihkk16q2r5q4ageuprf",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "from typing import Tuple, Union, List, Callable, Optional\n",
        "from tqdm import tqdm\n",
        "from itertools import islice\n",
        "import pathlib\n",
        "import dataclasses\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import distributions\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torchaudio\n",
        "from IPython import display as display_\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "rf77fj57is6s7r7roaf2d",
        "id": "812GwLfqqUYf"
      },
      "source": [
        "# Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "gtxjm07h83vodd6nkbb4en",
        "id": "i1DuQIyRqUYf"
      },
      "source": [
        "In this notebook we will implement a model for finding a keyword in a stream.\n",
        "\n",
        "We will implement the version with CRNN because it is easy and improves the model. \n",
        "(from https://www.dropbox.com/s/22ah2ba7dug6pzw/KWS_Attention.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PdhApeEh9pH",
        "cellId": "r1x3mjzmmhawdqs4bpgaf",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "@dataclasses.dataclass\n",
        "class TaskConfig:\n",
        "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
        "    batch_size: int = 128\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 1e-5\n",
        "    num_epochs: int = 50\n",
        "    n_mels: int = 40\n",
        "    cnn_out_channels: int = 8\n",
        "    kernel_size: Tuple[int, int] = (5, 20)\n",
        "    stride: Tuple[int, int] = (2, 8)\n",
        "    hidden_size: int = 64\n",
        "    gru_num_layers: int = 2\n",
        "    bidirectional: bool = False\n",
        "    num_classes: int = 2\n",
        "    sample_rate: int = 16000\n",
        "    device: torch.device = torch.device(\n",
        "        'cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "pcmenfvfw1jqunvnkope7i",
        "id": "KA1gPmE1h9pI"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2N8zcx9MF1X",
        "cellId": "jeq6um3hkgpkkc3i0notm",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95504fa0-f098-49e7-cd2e-96528f020562"
      },
      "source": [
        "!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
        "!mkdir speech_commands && tar -C speech_commands -xvzf speech_commands_v0.01.tar.gz 1> log"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-16 18:08:41--  http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 142.251.8.128, 2404:6800:4008:c01::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|142.251.8.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1489096277 (1.4G) [application/gzip]\n",
            "Saving to: ‘speech_commands_v0.01.tar.gz’\n",
            "\n",
            "speech_commands_v0. 100%[===================>]   1.39G  29.5MB/s    in 42s     \n",
            "\n",
            "2021-11-16 18:09:24 (34.0 MB/s) - ‘speech_commands_v0.01.tar.gz’ saved [1489096277/1489096277]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12wBTK0mNUsG",
        "cellId": "fwqjo2jldkhlq69dhyk6r",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "class SpeechCommandDataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        transform: Optional[Callable] = None,\n",
        "        path2dir: str = None,\n",
        "        keywords: Union[str, List[str]] = None,\n",
        "        csv: Optional[pd.DataFrame] = None\n",
        "    ):        \n",
        "        self.transform = transform\n",
        "\n",
        "        if csv is None:\n",
        "            path2dir = pathlib.Path(path2dir)\n",
        "            keywords = keywords if isinstance(keywords, list) else [keywords]\n",
        "            \n",
        "            all_keywords = [\n",
        "                p.stem for p in path2dir.glob('*')\n",
        "                if p.is_dir() and not p.stem.startswith('_')\n",
        "            ]\n",
        "\n",
        "            triplets = []\n",
        "            for keyword in all_keywords:\n",
        "                paths = (path2dir / keyword).rglob('*.wav')\n",
        "                if keyword in keywords:\n",
        "                    for path2wav in paths:\n",
        "                        triplets.append((path2wav.as_posix(), keyword, 1))\n",
        "                else:\n",
        "                    for path2wav in paths:\n",
        "                        triplets.append((path2wav.as_posix(), keyword, 0))\n",
        "            \n",
        "            self.csv = pd.DataFrame(\n",
        "                triplets,\n",
        "                columns=['path', 'keyword', 'label']\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.csv = csv\n",
        "    \n",
        "    def __getitem__(self, index: int):\n",
        "        instance = self.csv.iloc[index]\n",
        "\n",
        "        path2wav = instance['path']\n",
        "        wav, sr = torchaudio.load(path2wav)\n",
        "        wav = wav.sum(dim=0)\n",
        "        \n",
        "        if self.transform:\n",
        "            wav = self.transform(wav)\n",
        "\n",
        "        return {\n",
        "            'wav': wav,\n",
        "            'keywors': instance['keyword'],\n",
        "            'label': instance['label']\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1rVkT81Pk90",
        "cellId": "6dav03muedps7xdww9e3n",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "dataset = SpeechCommandDataset(\n",
        "    path2dir='speech_commands', keywords=TaskConfig.keyword\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFwhAXdfQLIA",
        "cellId": "8a59qgb849vxoi3trgwry",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "698c7894-0ee4-4e9a-a628-1165810f5367"
      },
      "source": [
        "#!g1.1\n",
        "dataset.csv.sample(5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>keyword</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50866</th>\n",
              "      <td>speech_commands/bird/b6091c84_nohash_0.wav</td>\n",
              "      <td>bird</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1003</th>\n",
              "      <td>speech_commands/no/0bd689d7_nohash_0.wav</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38126</th>\n",
              "      <td>speech_commands/go/be7a5b2d_nohash_3.wav</td>\n",
              "      <td>go</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>speech_commands/no/25040e85_nohash_0.wav</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31651</th>\n",
              "      <td>speech_commands/down/bdee441c_nohash_0.wav</td>\n",
              "      <td>down</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             path keyword  label\n",
              "50866  speech_commands/bird/b6091c84_nohash_0.wav    bird      0\n",
              "1003     speech_commands/no/0bd689d7_nohash_0.wav      no      0\n",
              "38126    speech_commands/go/be7a5b2d_nohash_3.wav      go      0\n",
              "144      speech_commands/no/25040e85_nohash_0.wav      no      0\n",
              "31651  speech_commands/down/bdee441c_nohash_0.wav    down      0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "vhorjvit57etpzytpvxe",
        "id": "LUxfDJw1qUYi"
      },
      "source": [
        "### Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmkxPWQqUYe",
        "jupyter": {
          "source_hidden": true
        },
        "cellId": "5dwg9j4f2zd63p4b8ouzkx",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "class AugsCreation:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.background_noises = [\n",
        "            'speech_commands/_background_noise_/white_noise.wav',\n",
        "            'speech_commands/_background_noise_/dude_miaowing.wav',\n",
        "            'speech_commands/_background_noise_/doing_the_dishes.wav',\n",
        "            'speech_commands/_background_noise_/exercise_bike.wav',\n",
        "            'speech_commands/_background_noise_/pink_noise.wav',\n",
        "            'speech_commands/_background_noise_/running_tap.wav'\n",
        "        ]\n",
        "\n",
        "        self.noises = [\n",
        "            torchaudio.load(p)[0].squeeze()\n",
        "            for p in self.background_noises\n",
        "        ]\n",
        "\n",
        "    def add_rand_noise(self, audio):\n",
        "\n",
        "        # randomly choose noise\n",
        "        noise_num = torch.randint(low=0, high=len(\n",
        "            self.background_noises), size=(1,)).item()\n",
        "        noise = self.noises[noise_num]\n",
        "\n",
        "        noise_level = torch.Tensor([1])  # [0, 40]\n",
        "\n",
        "        noise_energy = torch.norm(noise)\n",
        "        audio_energy = torch.norm(audio)\n",
        "        alpha = (audio_energy / noise_energy) * \\\n",
        "            torch.pow(10, -noise_level / 20)\n",
        "\n",
        "        start = torch.randint(\n",
        "            low=0,\n",
        "            high=max(int(noise.size(0) - audio.size(0) - 1), 1),\n",
        "            size=(1,)\n",
        "        ).item()\n",
        "        noise_sample = noise[start: start + audio.size(0)]\n",
        "\n",
        "        audio_new = audio + alpha * noise_sample\n",
        "        audio_new.clamp_(-1, 1)\n",
        "        return audio_new\n",
        "\n",
        "    def __call__(self, wav):\n",
        "        aug_num = torch.randint(low=0, high=4, size=(1,)).item()   # choose 1 random aug from augs\n",
        "        augs = [\n",
        "            lambda x: x,\n",
        "            lambda x: (x + distributions.Normal(0, 0.01).sample(x.size())).clamp_(-1, 1),\n",
        "            lambda x: torchaudio.transforms.Vol(.25)(x),\n",
        "            lambda x: self.add_rand_noise(x)\n",
        "        ]\n",
        "\n",
        "        return augs[aug_num](wav)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClWThxyYh9pM",
        "jupyter": {
          "source_hidden": true
        },
        "cellId": "meuiwn7oazfikzsxbzly",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "indexes = torch.randperm(len(dataset))\n",
        "train_indexes = indexes[:int(len(dataset) * 0.8)]\n",
        "val_indexes = indexes[int(len(dataset) * 0.8):]\n",
        "\n",
        "train_df = dataset.csv.iloc[train_indexes].reset_index(drop=True)\n",
        "val_df = dataset.csv.iloc[val_indexes].reset_index(drop=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDPLht5fqUYe",
        "jupyter": {
          "source_hidden": true
        },
        "cellId": "p01vkc8qf9gkb237ncp0g",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "# Sample is a dict of utt, word and label\n",
        "train_set = SpeechCommandDataset(csv=train_df, transform=AugsCreation())\n",
        "val_set = SpeechCommandDataset(csv=val_df)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "fnl456zbslwnh49oinqm3r",
        "id": "mmrJd8WIhkLP"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "tlv68756yafx4r4t1tj58",
        "id": "2vbPDqd6qUYj"
      },
      "source": [
        "### Sampler for oversampling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfnjRKo2qUYj",
        "cellId": "1u7a0d8vox8dnpcue32ev5",
        "trusted": true
      },
      "source": [
        "# We should provide to WeightedRandomSampler _weight for every sample_; by default it is 1/len(target)\n",
        "#!g1.1\n",
        "\n",
        "def get_sampler(target):\n",
        "    class_sample_count = np.array(\n",
        "        [len(np.where(target == t)[0]) for t in np.unique(target)])   # for every class count it's number of occ.\n",
        "    weight = 1. / class_sample_count\n",
        "    samples_weight = np.array([weight[t] for t in target])\n",
        "    samples_weight = torch.from_numpy(samples_weight)\n",
        "    samples_weigth = samples_weight.float()\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "    return sampler"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM8gLmHeqUYj",
        "cellId": "7iuad6ms0qb3xulehlyk2",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "train_sampler = get_sampler(train_set.csv['label'].values)\n",
        "val_sampler = get_sampler(val_set.csv['label'].values)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyBqbxp0h9pO",
        "cellId": "smjcccf204pp7a9vsyoz4g",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "class Collator:\n",
        "    \n",
        "    def __call__(self, data):\n",
        "        wavs = []\n",
        "        labels = []    \n",
        "\n",
        "        for el in data:\n",
        "            wavs.append(el['wav'])\n",
        "            labels.append(el['label'])\n",
        "\n",
        "        # torch.nn.utils.rnn.pad_sequence takes list(Tensors) and returns padded (with 0.0) Tensor\n",
        "        wavs = pad_sequence(wavs, batch_first=True)    \n",
        "        labels = torch.Tensor(labels).long()\n",
        "        return wavs, labels"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "e3agl2pdtefr791vlh3fdq",
        "id": "e8G9xPRVqUYk"
      },
      "source": [
        "###  Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wGBMcQiqUYk",
        "cellId": "spwlkkj9hzk5w1q6o2g0rw",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "# wtf is going on in this cell? Nothing special, just Datasphere things.\n",
        "\n",
        "# Here we are obliged to use shuffle=False because of our sampler with randomness inside.\n",
        "train_loader = DataLoader(train_set, batch_size=TaskConfig.batch_size,\n",
        "                          shuffle=False, collate_fn=Collator(),\n",
        "                          sampler=train_sampler,\n",
        "                          num_workers=0, pin_memory=True)\n",
        "\n",
        "val_loader = DataLoader(val_set, batch_size=TaskConfig.batch_size,\n",
        "                        shuffle=False, collate_fn=Collator(),\n",
        "                        sampler=val_sampler,\n",
        "                        num_workers=0, pin_memory=True)\n",
        "\n",
        "next(iter(train_loader))\n",
        "next(iter(val_loader))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=TaskConfig.batch_size,\n",
        "                          shuffle=False, collate_fn=Collator(),\n",
        "                          sampler=train_sampler,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "val_loader = DataLoader(val_set, batch_size=TaskConfig.batch_size,\n",
        "                        shuffle=False, collate_fn=Collator(),\n",
        "                        sampler=val_sampler,\n",
        "                        num_workers=2, pin_memory=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "8m19k1ebaclf0zu34jfdqp",
        "id": "kTlsn6cpqUYk"
      },
      "source": [
        "### Creating MelSpecs on GPU for speeeed: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRXMt6it56fW",
        "cellId": "qv7aki1mf9n3cgb7yw1edq",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "class LogMelspec:\n",
        "\n",
        "    def __init__(self, is_train, config):\n",
        "        # with augmentations\n",
        "        if is_train:\n",
        "            self.melspec = nn.Sequential(\n",
        "                torchaudio.transforms.MelSpectrogram(\n",
        "                    sample_rate=config.sample_rate,\n",
        "                    n_fft=400,\n",
        "                    win_length=400,\n",
        "                    hop_length=160,\n",
        "                    n_mels=config.n_mels\n",
        "                ),\n",
        "                torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
        "                torchaudio.transforms.TimeMasking(time_mask_param=35),\n",
        "            ).to(config.device)\n",
        "\n",
        "        # no augmentations\n",
        "        else:\n",
        "            self.melspec = torchaudio.transforms.MelSpectrogram(\n",
        "                sample_rate=config.sample_rate,\n",
        "                n_fft=400,\n",
        "                win_length=400,\n",
        "                hop_length=160,\n",
        "                n_mels=config.n_mels\n",
        "            ).to(config.device)\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # already on device\n",
        "        return torch.log(self.melspec(batch).clamp_(min=1e-9, max=1e9))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqkz4_gn8BiF",
        "cellId": "n8ldolyq3pd3poi57urr",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "melspec_train = LogMelspec(is_train=True, config=TaskConfig)\n",
        "melspec_val = LogMelspec(is_train=False, config=TaskConfig)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "wktprz3g8tvu0tfyljbri",
        "id": "zoAxmihY8yxr"
      },
      "source": [
        "### Quality measurment functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euwD1UyuqUYk",
        "cellId": "6mvseym8mg311lniraduo7m",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "# FA - true: 0, model: 1\n",
        "# FR - true: 1, model: 0\n",
        "\n",
        "def count_FA_FR(preds, labels):\n",
        "    FA = torch.sum(preds[labels == 0])\n",
        "    FR = torch.sum(labels[preds == 0])\n",
        "    \n",
        "    # torch.numel - returns total number of elements in tensor\n",
        "    return FA.item() / torch.numel(preds), FR.item() / torch.numel(preds)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHBUrkT1qUYk",
        "cellId": "ss889hn8kgg44rjgtglao",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "def get_au_fa_fr(probs, labels):\n",
        "    sorted_probs, _ = torch.sort(probs)\n",
        "    sorted_probs = torch.cat((torch.Tensor([0]), sorted_probs, torch.Tensor([1])))\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "        \n",
        "    FAs, FRs = [], []\n",
        "    for prob in sorted_probs:\n",
        "        preds = (probs >= prob) * 1\n",
        "        FA, FR = count_FA_FR(preds, labels)        \n",
        "        FAs.append(FA)\n",
        "        FRs.append(FR)\n",
        "    # plt.plot(FAs, FRs)\n",
        "    # plt.show()\n",
        "\n",
        "    # ~ area under curve using trapezoidal rule\n",
        "    return -np.trapz(FRs, x=FAs)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "qs18r8zb8vchmncnf5cxg",
        "id": "CcEP5cEZqUYl"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cP_pFIsy5p2",
        "cellId": "uyu1mpxdbafodddb7bpif",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb10da4-25e5-4be6-dfc0-ffb6e74eaff0"
      },
      "source": [
        "#!g1.1\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.energy = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, input):\n",
        "        energy = self.energy(input)\n",
        "        alpha = torch.softmax(energy, dim=-2)\n",
        "        return (input * alpha).sum(dim=-2)\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, config: TaskConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1, out_channels=config.cnn_out_channels,\n",
        "                kernel_size=config.kernel_size, stride=config.stride\n",
        "            ),\n",
        "            nn.Flatten(start_dim=1, end_dim=2),\n",
        "        )\n",
        "\n",
        "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
        "            config.stride[0] + 1\n",
        "        \n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
        "            hidden_size=config.hidden_size,\n",
        "            num_layers=config.gru_num_layers,\n",
        "            dropout=0.1,\n",
        "            bidirectional=config.bidirectional,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.attention = Attention(config.hidden_size)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        input = input.unsqueeze(dim=1)\n",
        "        conv_output = self.conv(input).transpose(-1, -2)\n",
        "        gru_output, _ = self.gru(conv_output)\n",
        "        contex_vector = self.attention(gru_output)\n",
        "        output = self.classifier(contex_vector)\n",
        "        return output\n",
        "config = TaskConfig()\n",
        "model = CRNN(config)\n",
        "model"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n",
              "    (1): Flatten(start_dim=1, end_dim=2)\n",
              "  )\n",
              "  (gru): GRU(144, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (attention): Attention(\n",
              "    (energy): Sequential(\n",
              "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmmSFvWaqUYn",
        "cellId": "vv6d6cq5u4jrgaawf0bkn",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "def train_epoch(model, opt, loader, log_melspec, device):\n",
        "    model.train()\n",
        "    for i, (batch, labels) in tqdm(enumerate(loader), total=len(loader)):\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        batch = log_melspec(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    return acc"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIeRbn4tqUYo",
        "cellId": "xm5zu307jchi5e69eh0f9",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "@torch.no_grad()\n",
        "def validation(model, loader, log_melspec, device):\n",
        "    model.eval()\n",
        "\n",
        "    val_losses, accs, FAs, FRs = [], [], [], []\n",
        "    all_probs, all_labels = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(loader)):\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        batch = log_melspec(batch)\n",
        "\n",
        "        output = model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(output, dim=-1)\n",
        "        loss = F.cross_entropy(output, labels)\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        all_probs.append(probs[:, 1].cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "        val_losses.append(loss.item())\n",
        "        accs.append(\n",
        "            torch.sum(argmax_probs == labels).item() /  # ???\n",
        "            torch.numel(argmax_probs)\n",
        "        )\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        FAs.append(FA)\n",
        "        FRs.append(FR)\n",
        "\n",
        "    # area under FA/FR curve for whole loader\n",
        "    au_fa_fr = get_au_fa_fr(torch.cat(all_probs, dim=0).cpu(), all_labels)\n",
        "    return au_fa_fr"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpyvKwp0k3IU",
        "cellId": "5ax8xr0rpc6fpljb5cqpcf",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "from collections import defaultdict\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "history = defaultdict(list)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "s18njnaikso8nmemgr8q6h",
        "id": "GSNW-nZCJ4Q0"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8sVpHNoocgA",
        "cellId": "w3yvjay5jafjpauo8akal",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d15341d6-5d3b-4ae1-a577-bfab3eeb84f7"
      },
      "source": [
        "#!g1.1\n",
        "config = TaskConfig()\n",
        "model = CRNN(config).to(config.device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "opt = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config.learning_rate,\n",
        "    weight_decay=config.weight_decay\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRNN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n",
            "    (1): Flatten(start_dim=1, end_dim=2)\n",
            "  )\n",
            "  (gru): GRU(144, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (attention): Attention(\n",
            "    (energy): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zedXm9dmINAE",
        "cellId": "ep204wnf3341jco3g4ml25",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe86ec7-414a-4911-a8cc-73a2fbfe0f95"
      },
      "source": [
        "#!g1.1\n",
        "sum([p.numel() for p in model.parameters()])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70443"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32oooz4lqUYo",
        "cellId": "bimoqzzcwaqekeouthq0dt",
        "trusted": true
      },
      "source": [
        "#!g1.1\n",
        "# TRAIN\n",
        "if False:\n",
        "    for n in range(1, TaskConfig.num_epochs + 1):\n",
        "\n",
        "        train_epoch(model, opt, train_loader,\n",
        "                    melspec_train, config.device)\n",
        "\n",
        "        au_fa_fr = validation(model, val_loader,\n",
        "                              melspec_val, config.device)\n",
        "        history['val_metric'].append(au_fa_fr)\n",
        "\n",
        "        clear_output()\n",
        "        plt.plot(history['val_metric'])\n",
        "        plt.ylabel('Metric')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "        print('END OF EPOCH', n)\n",
        "\n",
        "        if (au_fa_fr - 0.0002) <= 1e-5:\n",
        "            break\n",
        "    assert (history['val_metric'][-1] - 0.0002) <= 1e-5, \"Quality is not good enough\"\n",
        "    torch.save(model.state_dict(), \"model.pt\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "kzr0hipsbf0b4nb38svbff",
        "id": "WzeLiSTsoFa7"
      },
      "source": [
        "# Streaming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "898kudvyx7ib9ydt5xlb2t",
        "trusted": true,
        "id": "QbPje4hDoFa8"
      },
      "source": [
        "#!g1.1\n",
        "model.load_state_dict(torch.load(\"model.pt\"))\n",
        "model.eval();"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "yd1yf4fcvgcev28xwci6qs",
        "trusted": true,
        "id": "t_TX0FBQoFa9"
      },
      "source": [
        "#!g1.1\n",
        "class StreamingKWS(CRNN):\n",
        "    def  __init__(self, config, window_length, step=None):\n",
        "        super().__init__(config)\n",
        "        \n",
        "        self.window_len = window_length\n",
        "        if step is not None:\n",
        "            assert step % config.stride[1] == 0, f\"Step should be divisible by {config.stride[1]}\"\n",
        "            self.step = step\n",
        "        else:\n",
        "            self.step = config.stride\n",
        "        self.rnn_in_len = (window_length-config.kernel_size[1])//config.stride[1]+1\n",
        "        self.kernel_size = config.kernel_size\n",
        "        self.reset()\n",
        "\n",
        "    def forward(self, input, mode='whole'):\n",
        "        outputs=[]\n",
        "        if mode == 'whole':\n",
        "            input = input.unsqueeze(dim=0).unsqueeze(dim=0)\n",
        "            for i in range(self.kernel_size[1], input.size(-1), self.step):\n",
        "                input_ = input[:,:,:,i-self.kernel_size[1]:i]\n",
        "                # print(input_.shape)\n",
        "                conv_output = self.conv(input_).transpose(-1, -2)\n",
        "                # print(conv_output.shape, self.cnn_buffer.shape)\n",
        "                self.cnn_buffer = torch.cat((self.cnn_buffer, conv_output), dim=1)[:,-self.window_len:]\n",
        "                # print(self.cnn_buffer)\n",
        "                gru_output, self.hidden = self.gru(self.cnn_buffer, self.hidden)\n",
        "                self.rnn_buffer = torch.cat((self.rnn_buffer, gru_output), dim=1)[:,-self.rnn_in_len:]\n",
        "                \n",
        "                contex_vector = self.attention(self.rnn_buffer)\n",
        "                output = self.classifier(contex_vector)\n",
        "                output = F.softmax(output, dim=-1)\n",
        "                outputs.append(output[0,1].item())\n",
        "            self.reset()\n",
        "        return outputs\n",
        "    def reset(self):\n",
        "        self.cnn_buffer = torch.zeros((1, window_length, self.conv_out_frequency * config.cnn_out_channels), device=config.device)\n",
        "        self.rnn_buffer = torch.zeros((1, self.rnn_in_len, config.hidden_size), device=config.device)\n",
        "        self.hidden = None\n",
        "        \n",
        "#     def proccess_whole(self, input):\n",
        "        \n",
        "#     def _process_window(self, window):\n",
        "#         contex_vector = self.model.contex_vector(window)\n",
        "        \n",
        "#         probs, attention = F.softmax(output, dim=-1)\n",
        "#         return probs[0,1]\n",
        "    \n",
        "#     def contex_vector(self, input):\n",
        "#         input = input.unsqueeze(dim=1)\n",
        "#         conv_output = self.conv(input).transpose(-1, -2)\n",
        "#         gru_output, _ = self.gru(conv_output)\n",
        "#         contex_vector = self.attention(gru_output)\n",
        "#         return contex_vector\n",
        "#     def classify(self, contex_vector):\n",
        "#         output = self.classifier(contex_vector)\n",
        "#         return output"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "yk4g7uqfd5gqj9pi7mhprl",
        "trusted": true,
        "id": "4T_MXB3PoFa_"
      },
      "source": [
        "#!g1.1\n",
        "streaming_model = StreamingKWS(model.config, 30, 8).to(model.config.device)\n",
        "streaming_model.load_state_dict(torch.load(\"model.pt\"))\n",
        "streaming_model.eval();"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "qrkgirn8w8h2gfgs9q9itk",
        "trusted": true,
        "id": "EOTNTx5UoFbA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "b51afa4c-ff1d-445b-94f9-a284f07d3e2c"
      },
      "source": [
        "#!g1.1\n",
        "device=model.config.device\n",
        "for batch, labels in train_loader:\n",
        "    batch, labels = batch[0:8].flatten(), labels[0:8]\n",
        "    print(labels)\n",
        "#     batch = batch.unsqueeze(dim=0)\n",
        "    batch, labels = batch.to(device), labels.to(device)\n",
        "    batch = melspec_val(batch)\n",
        "    output = streaming_model(batch)\n",
        "#     plt.imshow(batch.cpu().numpy())\n",
        "    plt.plot(output)\n",
        "    plt.show()\n",
        "    print(output)\n",
        "    break"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 0, 0, 0, 1, 0, 1])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9953816533088684, 0.9917995929718018, 0.9646750688552856, 0.19458983838558197, 0.0025323175359517336, 0.000290459516691044, 9.909772779792547e-05, 6.8618894147221e-05, 0.00013620065874420106, 0.000190340171684511, 0.00027819297974929214, 0.0005054086213931441, 0.0006011399673298001, 0.0007397818844765425, 0.001161607331596315, 0.0035099349915981293, 0.2133338898420334, 0.982600748538971, 0.9990899562835693, 0.9992969036102295, 0.9984707236289978, 0.9798210859298706, 0.7446004152297974, 0.5369266867637634, 0.3794364929199219, 0.2623820900917053, 0.13750457763671875, 0.1620578169822693, 0.08963124454021454, 0.013533764518797398, 0.0008543903823010623, 0.0005262410268187523, 0.00222451682202518, 0.010112199932336807, 0.005984339397400618, 0.0015528631629422307, 0.004459827207028866, 0.003214967902749777, 0.0012155250879004598, 0.0014675117563456297, 0.002135118469595909, 0.0033893131185323, 0.02265576645731926, 0.03757866844534874, 0.009720614179968834, 0.0003697032807394862, 8.933033677749336e-05, 6.755383219569921e-05, 5.3906216635368764e-05, 4.372431794763543e-05, 4.330082447268069e-05, 5.407148273661733e-05, 0.0008414772455580533, 0.0012111114338040352, 0.0008097516838461161, 0.0005353287560865283, 0.00035993888741359115, 0.0004922954249195755, 0.0008269066456705332, 0.0011344741797074676, 0.0032139031682163477, 0.011991092935204506, 0.017249146476387978, 0.6592221260070801, 0.9889599084854126, 0.9987887740135193, 0.9998812675476074, 0.9999485015869141, 0.9999227523803711, 0.9990735054016113, 0.9776083827018738, 0.8966695070266724, 0.711434006690979, 0.44105300307273865, 0.32100749015808105, 0.2183312624692917, 0.1946401596069336, 0.2396993488073349, 0.06255483627319336, 0.001762310159392655, 0.00023724565107841045, 0.00010335451224818826, 4.528950739768334e-05, 3.386926618986763e-05, 3.390416532056406e-05, 3.62961545761209e-05, 5.692581908078864e-05, 0.00017980582197196782, 0.0004924907116219401, 0.001409127376973629, 0.013040684163570404, 0.016329249367117882, 0.18616114556789398, 0.9303460121154785, 0.9996713399887085, 0.9998471736907959, 0.9998056292533875, 0.9987014532089233]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "1zojars2lt8qqerd2zvnq",
        "id": "RrwODuGQoFbB"
      },
      "source": [
        "#!g1.1\n"
      ],
      "execution_count": 69,
      "outputs": []
    }
  ]
}