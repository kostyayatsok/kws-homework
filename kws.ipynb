{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seminar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhrn5O-qUYZ"
      },
      "source": [
        "# Import and misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "meO-Mp9jiAFC",
        "outputId": "4f06a169-1ef1-49c5-f849-9dae932f8eb2"
      },
      "source": [
        "!pip install torchaudio==0.9.1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==0.9.1\n",
            "  Downloading torchaudio-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting torch==1.9.1\n",
            "  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 7.0 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.1->torchaudio==0.9.1) (3.10.0.2)\n",
            "Installing collected packages: torch, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.10.0\n",
            "    Uninstalling torchaudio-0.10.0:\n",
            "      Successfully uninstalled torchaudio-0.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.9.1 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.9.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.1 torchaudio-0.9.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbUpoArCqUYa"
      },
      "source": [
        "from typing import Tuple, Union, List, Callable, Optional\n",
        "from tqdm import tqdm\n",
        "from itertools import islice\n",
        "import pathlib\n",
        "import dataclasses\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import distributions\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torchaudio\n",
        "from IPython import display as display_"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "812GwLfqqUYf"
      },
      "source": [
        "# Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1DuQIyRqUYf"
      },
      "source": [
        "In this notebook we will implement a model for finding a keyword in a stream.\n",
        "\n",
        "We will implement the version with CRNN because it is easy and improves the model. \n",
        "(from https://www.dropbox.com/s/22ah2ba7dug6pzw/KWS_Attention.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PdhApeEh9pH"
      },
      "source": [
        "@dataclasses.dataclass\n",
        "class TaskConfig:\n",
        "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
        "    batch_size: int = 128\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 1e-5\n",
        "    num_epochs: int = 20\n",
        "    n_mels: int = 40\n",
        "    cnn_out_channels: int = 8\n",
        "    kernel_size: Tuple[int, int] = (5, 20)\n",
        "    stride: Tuple[int, int] = (2, 8)\n",
        "    hidden_size: int = 64\n",
        "    gru_num_layers: int = 2\n",
        "    bidirectional: bool = False\n",
        "    num_classes: int = 2\n",
        "    sample_rate: int = 16000\n",
        "    device: torch.device = torch.device(\n",
        "        'cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA1gPmE1h9pI"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2N8zcx9MF1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7adf9cf-5348-4821-dff6-251e29a1b994"
      },
      "source": [
        "# !wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
        "# !mkdir speech_commands && tar -C speech_commands -xvzf speech_commands_v0.01.tar.gz 1> log"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-14 20:30:40--  http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 142.251.8.128, 2404:6800:4008:c01::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|142.251.8.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1489096277 (1.4G) [application/gzip]\n",
            "Saving to: ‘speech_commands_v0.01.tar.gz’\n",
            "\n",
            "speech_commands_v0. 100%[===================>]   1.39G  74.0MB/s    in 36s     \n",
            "\n",
            "2021-11-14 20:31:17 (39.4 MB/s) - ‘speech_commands_v0.01.tar.gz’ saved [1489096277/1489096277]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12wBTK0mNUsG"
      },
      "source": [
        "class SpeechCommandDataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        transform: Optional[Callable] = None,\n",
        "        path2dir: str = None,\n",
        "        keywords: Union[str, List[str]] = None,\n",
        "        csv: Optional[pd.DataFrame] = None\n",
        "    ):        \n",
        "        self.transform = transform\n",
        "\n",
        "        if csv is None:\n",
        "            path2dir = pathlib.Path(path2dir)\n",
        "            keywords = keywords if isinstance(keywords, list) else [keywords]\n",
        "            \n",
        "            all_keywords = [\n",
        "                p.stem for p in path2dir.glob('*')\n",
        "                if p.is_dir() and not p.stem.startswith('_')\n",
        "            ]\n",
        "\n",
        "            triplets = []\n",
        "            for keyword in all_keywords:\n",
        "                paths = (path2dir / keyword).rglob('*.wav')\n",
        "                if keyword in keywords:\n",
        "                    for path2wav in paths:\n",
        "                        triplets.append((path2wav.as_posix(), keyword, 1))\n",
        "                else:\n",
        "                    for path2wav in paths:\n",
        "                        triplets.append((path2wav.as_posix(), keyword, 0))\n",
        "            \n",
        "            self.csv = pd.DataFrame(\n",
        "                triplets,\n",
        "                columns=['path', 'keyword', 'label']\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.csv = csv\n",
        "    \n",
        "    def __getitem__(self, index: int):\n",
        "        instance = self.csv.iloc[index]\n",
        "\n",
        "        path2wav = instance['path']\n",
        "        wav, sr = torchaudio.load(path2wav)\n",
        "        wav = wav.sum(dim=0)\n",
        "        \n",
        "        if self.transform:\n",
        "            wav = self.transform(wav)\n",
        "\n",
        "        return {\n",
        "            'wav': wav,\n",
        "            'keywors': instance['keyword'],\n",
        "            'label': instance['label']\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1rVkT81Pk90"
      },
      "source": [
        "dataset = SpeechCommandDataset(\n",
        "    path2dir='speech_commands', keywords=TaskConfig.keyword\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DFwhAXdfQLIA",
        "outputId": "92c8ef89-97ac-43af-a29a-fab2c8cf6100"
      },
      "source": [
        "dataset.csv.sample(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>keyword</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>55254</th>\n",
              "      <td>speech_commands/go/89e59d18_nohash_1.wav</td>\n",
              "      <td>go</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9233</th>\n",
              "      <td>speech_commands/dog/cb802c63_nohash_0.wav</td>\n",
              "      <td>dog</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7493</th>\n",
              "      <td>speech_commands/left/d90b4138_nohash_0.wav</td>\n",
              "      <td>left</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44262</th>\n",
              "      <td>speech_commands/happy/093f65a1_nohash_0.wav</td>\n",
              "      <td>happy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42729</th>\n",
              "      <td>speech_commands/marvin/6727b579_nohash_0.wav</td>\n",
              "      <td>marvin</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               path keyword  label\n",
              "55254      speech_commands/go/89e59d18_nohash_1.wav      go      0\n",
              "9233      speech_commands/dog/cb802c63_nohash_0.wav     dog      0\n",
              "7493     speech_commands/left/d90b4138_nohash_0.wav    left      0\n",
              "44262   speech_commands/happy/093f65a1_nohash_0.wav   happy      0\n",
              "42729  speech_commands/marvin/6727b579_nohash_0.wav  marvin      0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUxfDJw1qUYi"
      },
      "source": [
        "### Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmkxPWQqUYe"
      },
      "source": [
        "class AugsCreation:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.background_noises = [\n",
        "            'speech_commands/_background_noise_/white_noise.wav',\n",
        "            'speech_commands/_background_noise_/dude_miaowing.wav',\n",
        "            'speech_commands/_background_noise_/doing_the_dishes.wav',\n",
        "            'speech_commands/_background_noise_/exercise_bike.wav',\n",
        "            'speech_commands/_background_noise_/pink_noise.wav',\n",
        "            'speech_commands/_background_noise_/running_tap.wav'\n",
        "        ]\n",
        "\n",
        "        self.noises = [\n",
        "            torchaudio.load(p)[0].squeeze()\n",
        "            for p in self.background_noises\n",
        "        ]\n",
        "\n",
        "    def add_rand_noise(self, audio):\n",
        "\n",
        "        # randomly choose noise\n",
        "        noise_num = torch.randint(low=0, high=len(\n",
        "            self.background_noises), size=(1,)).item()\n",
        "        noise = self.noises[noise_num]\n",
        "\n",
        "        noise_level = torch.Tensor([1])  # [0, 40]\n",
        "\n",
        "        noise_energy = torch.norm(noise)\n",
        "        audio_energy = torch.norm(audio)\n",
        "        alpha = (audio_energy / noise_energy) * \\\n",
        "            torch.pow(10, -noise_level / 20)\n",
        "\n",
        "        start = torch.randint(\n",
        "            low=0,\n",
        "            high=max(int(noise.size(0) - audio.size(0) - 1), 1),\n",
        "            size=(1,)\n",
        "        ).item()\n",
        "        noise_sample = noise[start: start + audio.size(0)]\n",
        "\n",
        "        audio_new = audio + alpha * noise_sample\n",
        "        audio_new.clamp_(-1, 1)\n",
        "        return audio_new\n",
        "\n",
        "    def __call__(self, wav):\n",
        "        aug_num = torch.randint(low=0, high=4, size=(1,)).item()   # choose 1 random aug from augs\n",
        "        augs = [\n",
        "            lambda x: x,\n",
        "            lambda x: (x + distributions.Normal(0, 0.01).sample(x.size())).clamp_(-1, 1),\n",
        "            lambda x: torchaudio.transforms.Vol(.25)(x),\n",
        "            lambda x: self.add_rand_noise(x)\n",
        "        ]\n",
        "\n",
        "        return augs[aug_num](wav)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClWThxyYh9pM"
      },
      "source": [
        "indexes = torch.randperm(len(dataset))\n",
        "train_indexes = indexes[:int(len(dataset) * 0.8)]\n",
        "val_indexes = indexes[int(len(dataset) * 0.8):]\n",
        "\n",
        "train_df = dataset.csv.iloc[train_indexes].reset_index(drop=True)\n",
        "val_df = dataset.csv.iloc[val_indexes].reset_index(drop=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDPLht5fqUYe"
      },
      "source": [
        "# Sample is a dict of utt, word and label\n",
        "train_set = SpeechCommandDataset(csv=train_df, transform=AugsCreation())\n",
        "val_set = SpeechCommandDataset(csv=val_df)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmrJd8WIhkLP"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vbPDqd6qUYj"
      },
      "source": [
        "### Sampler for oversampling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfnjRKo2qUYj"
      },
      "source": [
        "# We should provide to WeightedRandomSampler _weight for every sample_; by default it is 1/len(target)\n",
        "\n",
        "def get_sampler(target):\n",
        "    class_sample_count = np.array(\n",
        "        [len(np.where(target == t)[0]) for t in np.unique(target)])   # for every class count it's number of occ.\n",
        "    weight = 1. / class_sample_count\n",
        "    samples_weight = np.array([weight[t] for t in target])\n",
        "    samples_weight = torch.from_numpy(samples_weight)\n",
        "    samples_weigth = samples_weight.float()\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "    return sampler"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM8gLmHeqUYj"
      },
      "source": [
        "train_sampler = get_sampler(train_set.csv['label'].values)\n",
        "val_sampler = get_sampler(val_set.csv['label'].values)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyBqbxp0h9pO"
      },
      "source": [
        "class Collator:\n",
        "    \n",
        "    def __call__(self, data):\n",
        "        wavs = []\n",
        "        labels = []    \n",
        "\n",
        "        for el in data:\n",
        "            wavs.append(el['wav'])\n",
        "            labels.append(el['label'])\n",
        "\n",
        "        # torch.nn.utils.rnn.pad_sequence takes list(Tensors) and returns padded (with 0.0) Tensor\n",
        "        wavs = pad_sequence(wavs, batch_first=True)    \n",
        "        labels = torch.Tensor(labels).long()\n",
        "        return wavs, labels"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8G9xPRVqUYk"
      },
      "source": [
        "###  Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wGBMcQiqUYk"
      },
      "source": [
        "# Here we are obliged to use shuffle=False because of our sampler with randomness inside.\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=TaskConfig.batch_size,\n",
        "                          shuffle=False, collate_fn=Collator(),\n",
        "                          sampler=train_sampler,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "val_loader = DataLoader(val_set, batch_size=TaskConfig.batch_size,\n",
        "                        shuffle=False, collate_fn=Collator(),\n",
        "                        sampler=val_sampler,\n",
        "                        num_workers=2, pin_memory=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlsn6cpqUYk"
      },
      "source": [
        "### Creating MelSpecs on GPU for speeeed: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRXMt6it56fW"
      },
      "source": [
        "class LogMelspec:\n",
        "\n",
        "    def __init__(self, is_train, config):\n",
        "        # with augmentations\n",
        "        if is_train:\n",
        "            self.melspec = nn.Sequential(\n",
        "                torchaudio.transforms.MelSpectrogram(\n",
        "                    sample_rate=config.sample_rate,\n",
        "                    n_fft=400,\n",
        "                    win_length=400,\n",
        "                    hop_length=160,\n",
        "                    n_mels=config.n_mels\n",
        "                ),\n",
        "                torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
        "                torchaudio.transforms.TimeMasking(time_mask_param=35),\n",
        "            ).to(config.device)\n",
        "\n",
        "        # no augmentations\n",
        "        else:\n",
        "            self.melspec = torchaudio.transforms.MelSpectrogram(\n",
        "                sample_rate=config.sample_rate,\n",
        "                n_fft=400,\n",
        "                win_length=400,\n",
        "                hop_length=160,\n",
        "                n_mels=config.n_mels\n",
        "            ).to(config.device)\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # already on device\n",
        "        return torch.log(self.melspec(batch).clamp_(min=1e-9, max=1e9))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqkz4_gn8BiF"
      },
      "source": [
        "melspec_train = LogMelspec(is_train=True, config=TaskConfig)\n",
        "melspec_val = LogMelspec(is_train=False, config=TaskConfig)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoAxmihY8yxr"
      },
      "source": [
        "### Quality measurment functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euwD1UyuqUYk"
      },
      "source": [
        "# FA - true: 0, model: 1\n",
        "# FR - true: 1, model: 0\n",
        "\n",
        "def count_FA_FR(preds, labels):\n",
        "    FA = torch.sum(preds[labels == 0])\n",
        "    FR = torch.sum(labels[preds == 0])\n",
        "    \n",
        "    # torch.numel - returns total number of elements in tensor\n",
        "    return FA.item() / torch.numel(preds), FR.item() / torch.numel(preds)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHBUrkT1qUYk"
      },
      "source": [
        "def get_au_fa_fr(probs, labels):\n",
        "    sorted_probs, _ = torch.sort(probs)\n",
        "    sorted_probs = torch.cat((torch.Tensor([0]), sorted_probs, torch.Tensor([1])))\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "        \n",
        "    FAs, FRs = [], []\n",
        "    for prob in sorted_probs:\n",
        "        preds = (probs >= prob) * 1\n",
        "        FA, FR = count_FA_FR(preds, labels)        \n",
        "        FAs.append(FA)\n",
        "        FRs.append(FR)\n",
        "    # plt.plot(FAs, FRs)\n",
        "    # plt.show()\n",
        "\n",
        "    # ~ area under curve using trapezoidal rule\n",
        "    return -np.trapz(FRs, x=FAs)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcEP5cEZqUYl"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cP_pFIsy5p2",
        "outputId": "baa3467d-b1c9-4d5c-acd1-4723ed39f1b7"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.energy = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, input):\n",
        "        energy = self.energy(input)\n",
        "        alpha = torch.softmax(energy, dim=-2)\n",
        "        return (input * alpha).sum(dim=-2)\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, config: TaskConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1, out_channels=config.cnn_out_channels,\n",
        "                kernel_size=config.kernel_size, stride=config.stride\n",
        "            ),\n",
        "            nn.Flatten(start_dim=1, end_dim=2),\n",
        "        )\n",
        "\n",
        "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
        "            config.stride[0] + 1\n",
        "        \n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
        "            hidden_size=config.hidden_size,\n",
        "            num_layers=config.gru_num_layers,\n",
        "            dropout=0.1,\n",
        "            bidirectional=config.bidirectional,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.attention = Attention(config.hidden_size)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        input = input.unsqueeze(dim=1)\n",
        "        conv_output = self.conv(input).transpose(-1, -2)\n",
        "        gru_output, _ = self.gru(conv_output)\n",
        "        contex_vector = self.attention(gru_output)\n",
        "        output = self.classifier(contex_vector)\n",
        "        return output\n",
        "\n",
        "config = TaskConfig()\n",
        "model = CRNN(config)\n",
        "model"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n",
              "    (1): Flatten(start_dim=1, end_dim=2)\n",
              "  )\n",
              "  (gru): GRU(144, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (attention): Attention(\n",
              "    (energy): Sequential(\n",
              "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmmSFvWaqUYn"
      },
      "source": [
        "def train_epoch(model, opt, loader, log_melspec, device):\n",
        "    model.train()\n",
        "    for i, (batch, labels) in tqdm(enumerate(loader), total=len(loader)):\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        batch = log_melspec(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    return acc"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIeRbn4tqUYo"
      },
      "source": [
        "@torch.no_grad()\n",
        "def validation(model, loader, log_melspec, device):\n",
        "    model.eval()\n",
        "\n",
        "    val_losses, accs, FAs, FRs = [], [], [], []\n",
        "    all_probs, all_labels = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(loader)):\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        batch = log_melspec(batch)\n",
        "\n",
        "        output = model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(output, dim=-1)\n",
        "        loss = F.cross_entropy(output, labels)\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        all_probs.append(probs[:, 1].cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "        val_losses.append(loss.item())\n",
        "        accs.append(\n",
        "            torch.sum(argmax_probs == labels).item() /  # ???\n",
        "            torch.numel(argmax_probs)\n",
        "        )\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        FAs.append(FA)\n",
        "        FRs.append(FR)\n",
        "\n",
        "    # area under FA/FR curve for whole loader\n",
        "    au_fa_fr = get_au_fa_fr(torch.cat(all_probs, dim=0).cpu(), all_labels)\n",
        "    return au_fa_fr"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpyvKwp0k3IU"
      },
      "source": [
        "from collections import defaultdict\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "history = defaultdict(list)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSNW-nZCJ4Q0"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8sVpHNoocgA",
        "outputId": "deeed91e-edfe-40d3-b810-3e06905f2a5a"
      },
      "source": [
        "config = TaskConfig()\n",
        "model = CRNN(config).to(config.device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "opt = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config.learning_rate,\n",
        "    weight_decay=config.weight_decay\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRNN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n",
            "    (1): Flatten(start_dim=1, end_dim=2)\n",
            "  )\n",
            "  (gru): GRU(144, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (attention): Attention(\n",
            "    (energy): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zedXm9dmINAE",
        "outputId": "bcbdc49f-8b90-41bd-f6a9-2964592e8494"
      },
      "source": [
        "sum([p.numel() for p in model.parameters()])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70443"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt2kjqC-IobK"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "32oooz4lqUYo",
        "scrolled": false,
        "outputId": "debc5f47-e019-4b5c-d58e-802ba277bc77"
      },
      "source": [
        "# TRAIN\n",
        "\n",
        "for n in range(TaskConfig.num_epochs):\n",
        "\n",
        "    train_epoch(model, opt, train_loader,\n",
        "                melspec_train, config.device)\n",
        "\n",
        "    au_fa_fr = validation(model, val_loader,\n",
        "                          melspec_val, config.device)\n",
        "    history['val_metric'].append(au_fa_fr)\n",
        "\n",
        "    clear_output()\n",
        "    plt.plot(history['val_metric'])\n",
        "    plt.ylabel('Metric')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    print('END OF EPOCH', n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3hddZ3v8fc3t53m0lxaml522rS0CG1JCpSWi4MFvNQbdeboWEQGZ9TO4wFHR5/xgJ6DZ3RwdGYOOCp47IiCikAHbz3KoGgJqEBbLm2hFCS0pW0otPSWJmnu3/PHXikh5J6srL12Pq/n2U/WXnutnU/6tPn0t3/rYu6OiIjIUGVFHUBEROJFxSEiIsOi4hARkWFRcYiIyLCoOEREZFhyog4wHqZOnepVVVUj2repqYnCwsKxDRSSOGWFeOWNU1aIV944ZYV45R1N1scff/xVdz+lzxfdPeMf55xzjo/UAw88MOJ9x1ucsrrHK2+csrrHK2+csrrHK+9osgKPeT+/U/VRlYiIDIuKQ0REhkXFISIiw6LiEBGRYVFxiIjIsKg4RERkWFQcIiIyLCqOfrR1dPHt2hd4+tWOqKOIiKQVFUc/crONtQ+9wMb9nVFHERFJKyqOfpgZ1clSdh1TcYiI9KTiGEBNsoT6Rqe5TR9XiYh0U3EMoDpZigPbX2qIOoqISNpQcQygurIEgK17j0acREQkfag4BjCtOJ/yfGPbvmNRRxERSRsqjkHMLcli6z6NOEREuqk4BjG3JIsXDzVztLkt6igiImlBxTGIeSXZAPq4SkQkoOIYxJzJqT+ibfq4SkQEUHEMqjDXmDe1kK0acYiIACqOIalOlmjEISISUHEMQXWylFcaWnmloSXqKCIikVNxDEGNTgQUETlJxTEEi2aWkJ1lOp9DRAQVx5Dk52bzpopiHZIrIoKKY8hqKkvYtu8Y7h51FBGRSKk4hqg6WcqxE+28eKg56igiIpFScQxRdTKYINc8h4hMcCqOITqtophETpbmOURkwlNxDFFudhaLZk7WiYAiMuGpOIahprKUp+sb6OjsijqKiEhkVBzDUJMs5UR7J88faIw6iohIZFQcw9A9Qa6Pq0RkIlNxDEPVlEKK83N0pVwRmdBUHMOQlWW6Uq6ITHgqjmGqTpby7P7jtLR3Rh1FRCQSKo5hqkmW0NHl7NjfEHUUEZFIqDiGqaayFNA9yEVk4gq1OMxspZk9Z2Z1ZnZtH68nzOzu4PWNZlbV47XrgvXPmdk7eu2XbWZPmtkvw8zfl+mT8zmlOKFLj4jIhBVacZhZNnAz8E5gIXC5mS3stdlHgSPuPh+4CfhasO9CYDWwCFgJ3BK8X7dPATvCyj4QM6MmWaKbOonIhBXmiGMZUOfuO929DbgLWNVrm1XA7cHyPcClZmbB+rvcvdXddwF1wfthZkng3cB3Q8w+oOpkKTtfbeJ4S3tUEUREIhNmccwC9vZ4vi9Y1+c27t4BHAOmDLLv14HPAZFd96M6WYI7PFWveQ4RmXhyog4wHGb2HuCAuz9uZisG2XYNsAagoqKC2traEX3PxsbGN+x7vC11M6efP/gEbXvzRvS+YegrazqLU944ZYV45Y1TVohX3rCyhlkc9UBlj+fJYF1f2+wzsxygBDg0wL6XAZeZ2buAfGCymf3I3T/c+5u7+1pgLcDSpUt9xYoVI/ohamtr6Wvff9mygcZECStWnDOi9w1Df1nTVZzyxikrxCtvnLJCvPKGlTXMj6o2AwvMbK6Z5ZGa7F7fa5v1wFXB8vuBDZ66N+t6YHVw1NVcYAGwyd2vc/eku1cF77ehr9IYDzXJUrbu1UdVIjLxhFYcwZzFNcCvSR0Btc7dt5vZl8zssmCzW4EpZlYHfAa4Nth3O7AOeAa4D7ja3dPqVO2aZCn1R09wqLE16igiIuMq1DkOd78XuLfXuut7LLcAH+hn3xuAGwZ471qgdixyjsRrV8o9xsWnT4sqhojIuNOZ4yO0eFYJWQZbdD6HiEwwKo4RKkzkMH9aka6UKyITjopjFKqTpWzbd4zUfL6IyMSg4hiFmmQJh5raqD96IuooIiLjRsUxCtVJXSlXRCYeFcconD6jmLzsLF0pV0QmFBXHKCRysjljRjHbdCKgiEwgKo5Rqk6W8lT9Mbq6NEEuIhODimOUqpMlNLZ2sPPVxqijiIiMCxXHKHXfSlbXrRKRiULFMUqnnlJEQV62TgQUkQlDxTFK2VnG4lklbNUhuSIyQag4xsCSylKe2d9AW0dkNyUUERk3Ko4xUJ0soa2jiz+9cjzqKCIioVNxjIGa4AxynQgoIhOBimMMJMsmUVaQy1ZdYl1EJgAVxxgws5NXyhURyXQqjjFSkyzhT68cp7mtI+ooIiKhUnGMkepkKV0O219qiDqKiEioVBxjpLoydQ9yzXOISKZTcYyRacX5zCzJ1zyHiGQ8FccYSk2Qa8QhIplNxTGGqitL2H2omaPNbVFHEREJjYpjDNXoVrIiMgGoOMbQ4lmpCXJ9XCUimUzFMYZKJuUyb2qhrpQrIhlNxTHGaio1QS4imU3FMcaqkyW80tDKKw0tUUcREQmFimOMVXdfKVcnAopIhlJxjLFFMyeTk2U6skpEMpaKY4zl52ZzWkWx7s0hIhlLxRGCmsoStu07hrtHHUVEZMypOEJQnSzl2Il2XjzUHHUUEZExp+IIQXUyuFKuPq4SkQyk4gjBaRXF5OdmaYJcRDJSqMVhZivN7DkzqzOza/t4PWFmdwevbzSzqh6vXResf87M3hGsyzezTWa21cy2m9k/hpl/pHKzs1g0s0QnAopIRgqtOMwsG7gZeCewELjczBb22uyjwBF3nw/cBHwt2HchsBpYBKwEbgnerxW4xN1rgCXASjM7L6yfYTSqkyU8Xd9AR2dX1FFERMZUmCOOZUCdu+909zbgLmBVr21WAbcHy/cAl5qZBevvcvdWd98F1AHLPKUx2D43eKTloUs1yVJOtHdSd7Bx8I1FRGIkJ8T3ngXs7fF8H7C8v23cvcPMjgFTgvWP9tp3FpwcyTwOzAdudveNfX1zM1sDrAGoqKigtrZ2RD9EY2PjiPZtaUqNNNb9diMXJXNH9L2Ha6RZoxKnvHHKCvHKG6esEK+8YWUNszhC4e6dwBIzKwV+ZmaL3f3pPrZbC6wFWLp0qa9YsWJE36+2tpaR7NvV5dyw+Te0FE5nxYozR/S9h2ukWaMSp7xxygrxyhunrBCvvGFlDfOjqnqgssfzZLCuz23MLAcoAQ4NZV93Pwo8QGoOJO1kZRnVSU2Qi0jmCbM4NgMLzGyumeWRmuxe32ub9cBVwfL7gQ2eOt16PbA6OOpqLrAA2GRmpwQjDcxsEvA24NkQf4ZRqUmW8uz+47S0d0YdRURkzIT2UVUwZ3EN8GsgG/ieu283sy8Bj7n7euBW4IdmVgccJlUuBNutA54BOoCr3b3TzGYAtwfzHFnAOnf/ZVg/w2hVJ0vp6HJ27G/grNllUccRERkToc5xuPu9wL291l3fY7kF+EA/+94A3NBr3TbgrLFPGo6ayu5byR5TcYhIxtCZ4yGaPjmfU4oTuvSIiGQUFUeIzIyaZIkuPSIiGUXFEbLqZCkvHGzkeEt71FFERMaEiiNk1ckS3OGpeo06RCQzDKk4zOzPzaykx/NSM3tfeLEyR01wD3J9XCUimWKoI44vuvvJ33zByXdfDCdSZikrzGN2eYFOBBSRjDHU4uhru9hdriQq1ckStu7ViENEMsNQi+MxM7vRzE4NHjeSutCgDEFNspT6oyc41NgadRQRkVEbanF8EmgD7g4ercDVYYXKNN23ktU8h4hkgiF93OTuTcAb7uAnQ7N4VglZlroH+cWnT4s6jojIqAxYHGb2dXf/tJn9P/q4YZK7XxZasgxSmMhh/rQitu7VBLmIxN9gI44fBl//Lewgma4mWcqGZw/g7qRucigiEk8DFoe7Px5ciXaNu18xTpkyUnVlKf/5+D7qj54gWVYQdRwRkREbdHI8uOPenOCeGjJCNZogF5EMMdRzMXYCfzSz9UBT90p3vzGUVBno9OmTycvOYuu+o7zrzBlRxxERGbGhFscLwSMLKA7WvWGyXPqXl5PFGTOK2aYTAUUk5oZaHM+4+3/2XGFmfd6ASfpXnSzl50/W09XlZGVpglxE4mmoJwBeN8R1MoDqZAnHWzvY+WrT4BuLiKSpwc7jeCfwLmCWmX2jx0uTSd0LXIahpjJ1pdyte48yf1pRxGlEREZmsBHHS8BjQAupa1N1P9YD7wg3WuY59ZQiCvOydaVcEYm1wc7j2ApsNbMfB9vOdvfnxiVZBsrOMhbPKmGrDskVkRgb6hzHSmALcB+AmS0JDs2VYaqpLOWZ/Q20dXRFHUVEZESGWhz/G1gGHAVw9y3A3JAyZbTqZAltHV3s2N8QdRQRkREZanG097wDYEDncYzA+fOmkJ+bxQ8eeTHqKCIiIzLU4thuZh8Css1sgZl9E3g4xFwZa0pRgg8vn8PPt9SzW4flikgMDedGTotI3cDpTqAB+HRYoTLdmrfMIzfb+OaGuqijiIgM25CKw92b3f0L7n6uuy8NllvCDpepphXna9QhIrE12AmAAx45pRs5jdyat8zjRxtf5Jsb6vg/f1kTdRwRkSEb7FpV5wN7SX08tRHQBZbGSPeo4/sP7+aTl8ynamph1JFERIZksI+qpgOfBxYD/w68DXjV3R909wfDDpfpNNchInE0YHG4e6e73+fuVwHnAXVArZldMy7pMpzmOkQkjgadHDezhJn9BfAj4GrgG8DPwg42UfztW07VqENEYmXA4jCzHwCPAGcD/xgcVfVld68fl3QTwCnFCa48T6MOEYmPwUYcHwYWAJ8CHjazhuBx3Mx0zYwxsuai1KjjGxuejzqKiMigBpvjyHL34uAxucej2N0nj1fITHdy1PFkPbs06hCRNDfUM8dHxMxWmtlzZlZnZtf28XrCzO4OXt9oZlU9XrsuWP+cmb0jWFdpZg+Y2TNmtt3MPhVm/vG05qJTycvJ4psadYhImgutOMwsG7gZeCewELjczBb22uyjwBF3nw/cBHwt2HchsJrUZU5WArcE79cBfNbdF5I6yuvqPt4zljTqEJG4CHPEsQyoc/ed7t4G3AWs6rXNKuD2YPke4FIzs2D9Xe7e6u67SB0GvMzd97v7EwDufhzYAcwK8WcYVxp1iEgcDHbm+GjMInXWebd9wPL+tnH3DjM7BkwJ1j/aa9/XFUTwsdZZpM5ofwMzWwOsAaioqKC2tnZEP0RjY+OI9x2JFbOy+NkT9SwrPMz0wuH1+nhnHa045Y1TVohX3jhlhXjlDStrmMURGjMrAn4CfNrd+zy6y93XAmsBli5d6itWrBjR96qtrWWk+47EonNaqf2XDWxqKufGdy8Z1r7jnXW04pQ3TlkhXnnjlBXilTesrGF+VFUPVPZ4ngzW9bmNmeUAJcChgfY1s1xSpXGHu/80lOQR0lyHiKS7MItjM7DAzOaaWR6pye7eV9tdD1wVLL8f2ODuHqxfHRx1NZfUuSSbgvmPW4Ed7n5jiNkjpbkOEUlnoRWHu3cA1wC/JjWJvc7dt5vZl8ys+3LstwJTzKwO+AxwbbDvdmAd8AxwH3C1u3cCFwJXApeY2Zbg8a6wfoaoaNQhIuks1DkOd78XuLfXuut7LLcAH+hn3xuAG3qt+wMT5NLuay46lR8++iLf3PA8N/7l8OY6RETCFOoJgDJyPUcdOw82Rh1HROQkFUca657r+JaunCsiaUTFkcZOKU7wV+dX8fMtGnWISPpQcaS5NRfN06hDRNKKiiPNTS3SqENE0ouKIwY06hCRdKLiiAGNOkQknag4YkKjDhFJFyqOmNCoQ0TShYojRjTqEJF0oOKIEY06RCQdqDhipnvU8U2NOkQkIiqOmOkedfxiSz0vaNQhIhFQccSQ5jpEJEoqjhjSqENEoqTiiKk1F80jkZOtUYeIjDsVR0ylRh1zNOoQkXGn4oixj2vUISIRUHHEmEYdIhIFFUfMadQhIuNNxRFzPUcd+xu7oo4jIhOAiiMDdI861r/QFnUUEZkAVBwZYGpRgqsuqOKR/Z187b5n6ezyqCOJSAZTcWSIz7ztNC6uzOHbtS/w8R88xvGW9qgjiUiGUnFkiLycLK5alODL71vMQ386yJ/f8jC7Xm2KOpaIZCAVR4a58rw5/OhjyznU2Mqqb/2Bh/50MOpIIpJhVBwZ6Lx5U1h/zZuZWTqJj3x/E9/9/U7cNe8hImNDxZGhKssL+MknLuDtC6fzT7/awT/cs43Wjs6oY4lIBlBxZLDCRA63XHE2n37rAu55fB+r1z7KgYaWqGOJSMypODJcVpbx6beexrevOJvnXj7OZd/6I1v3Ho06lojEmIpjgnjnmTP4yScuICfb+MB3HuHnT9ZHHUlEYkrFMYGcMWMy6695M2dVlvLpu7fwz/+1QycLisiwqTgmmPLCPH70seVced4cvvPgTj52+2YadLKgiAyDimMCys3O4svvW8wNf76Y3z//Ku+7+Y/s1GXZRWSIQi0OM1tpZs+ZWZ2ZXdvH6wkzuzt4faOZVfV47bpg/XNm9o4e679nZgfM7Okws08EVyyfwx0fW87R5nZW3fxHap87EHUkEYmB0IrDzLKBm4F3AguBy81sYa/NPgoccff5wE3A14J9FwKrgUXASuCW4P0AbgvWyRhYPm8K66+5kGRZAX9z22b+4yGdLCgiAwtzxLEMqHP3ne7eBtwFrOq1zSrg9mD5HuBSM7Ng/V3u3uruu4C64P1w94eAwyHmnnCSZQX85BPns3LxdG64dwefXbeVlnadLCgifcsJ8b1nAXt7PN8HLO9vG3fvMLNjwJRg/aO99p01nG9uZmuANQAVFRXU1tYOZ/eTGhsbR7zveBtt1g/MdBIncvnpk/Vs2bmfT56VoCw/vP9bTKQ/2/EWp7xxygrxyhtW1jCLI1LuvhZYC7B06VJfsWLFiN6ntraWke473sYi68UXwzuefpnPrNvCPz/exXeuPIuzZpeNTcBeJtqf7XiKU944ZYV45Q0ra5jFUQ9U9nieDNb1tc0+M8sBSoBDQ9xXQrJy8XSqpl7Ax3/wGB9c+ygfuaCKmSX5lBXmMaUwQXlh3slHXo4OzBOZaMIsjs3AAjObS+qX/mrgQ722WQ9cBTwCvB/Y4O5uZuuBH5vZjcBMYAGwKcSs0svp0yfzi6vfzN/fvYX/+P1O+psvL07kUBaUyJTCvKBcUs/L+lhXlMghNY0lInEVWnEEcxbXAL8GsoHvuft2M/sS8Ji7rwduBX5oZnWkJrxXB/tuN7N1wDNAB3C1u3cCmNmdwApgqpntA77o7reG9XNMZOWFedz+N8vo7HKONrdxpLmNQ41tHG5q43BzG4cb2zjUlFp/uKmN/cdaeGZ/A4ea2mjr6OrzPfOysygrzGVKYYJTJ7Vx5tJWphQlxvknE5HRCHWOw93vBe7tte76HsstwAf62fcG4IY+1l8+xjFlENlZxpSiBFOKEsyfNvj27k5zWyeHm4JiCb4ebmrlcFM7h5taqT96gl/WNXD/1zbwwaWVfPyieSTLCsL/YURk1DJ2clyiY2YUJnIoTORQWd5/Gfz4lxt48sQU7ti4hzs27uGyJTP5xFtOZUFF8TimFZHhUnFIZGYWZfGh99Tw9287je/+fhd3btrDT5+o5+0LK/jEilNDO5pLREZHxSGRm1k6ievfu5BrLpnPbQ/v5vaHd/ObZ17h/HlT+O8Xn8qb50/VhLpIGtGxlJI2ygvz+MzbTuOP117CF951BjtfbeTKWzdx2bf+yH89tZ8uXQJeJC2oOCTtFCVy+PhF83jocxfz1b84k+Mt7Xzijid4600Psu6xvf0esSUi40PFIWkrkZPN6mWz+d1nV/CtD51Ffk42n7tnG2/51we49Q+7aG7riDqiyISk4pC0l51lvKd6Jr/6uzdz21+fy+zyAr78y2e48Ksb+PffPs/R5raoI4pMKJocl9gwM1a8aRor3jSNx188wrdr67jpt3/iOw+9wIeWzeZjfzaP6SX5UccUyXgqDomlc+aU8d2rzuXZlxv4zoM7+f7Du/nBIy/yF2fP4srz57BoZknUEUUylopDYu306ZO56YNL+MzbTmPtQztZ99he7tq8l5pkCZcvm817a2ZSmNBfc5GxpDkOyQiV5QV8+X2L2fj5S/niexdyor2Ta3/6FMtu+C2f/9lTPF1/LOqIIhlD/xWTjFJakMdfXziXj1xQxRN7jvDjjXv5yeP7+PHGPZw5KzUKuWzJTIo0ChEZMf3rkYxkZpwzp5xz5pRz/XsX8ost9fx44x4+/7On+KdfPcOqJTNZfe5sqpMlOitdZJhUHJLxSibl8lfnV3HleXPYsvcod27aw8+ffIk7N+1l4YzJXL58NquWzGRyfm7UUUViQcUhE4aZcdbsMs6aXcb/fM9CfrHlJX68cQ//6+dP85Vf7eC9NTO4fNlsllSWahQiMgAVh0xIk/NzufK8OXx4+Wy27TvGnZv2sH7rS6x7bB+nTy/mQ8tns2rJLEomaRQi0puKQyY0M6OmspSaylK+8O4zWL/1Je7ctIfrf7Gdr9y7g3efOZMPLa/Ee907t6vLaevsorW9i9aOTlo7uoJHsNzeFbyeet7W6/W2YHlqUYJzq8o5Y8ZksrM0ypF4UHGIBIrzc7li+RyuWD6Hp/Yd487Ne/jFk/X85Il9FOdB3h/uP/nLv71z9Ffqzc4yOoMr/hYncjh7ThnL5pZzblU51ckS8nOzR/09RMKg4hDpw5nJEs5MnskX3pUahdy7cQdzKqeTyMkmLyeLRE4WiZxsEjlZrz3PzSYvO4tE7utf715+bbss8rKzyMnOov7oCTbvOsym3YfZvOsw//rr54DUvdlrKks4t6qcc+eWc86cMk3eS9pQcYgMoDCRw+XLZjOjeScrVpw55u8/q3QSs86axfvOmgXA4aY2Htt9mM27D7Np9xG+89BObql9gSxLnSW/bG75yVHJKcWJMc8jMhQqDpE0Ul6Yx9sXTefti6YD0NzWwZN7jrJpV6pM7tq8h9se3g3A3KmFnFtVxrlVqTKZXV6go8FkXKg4RNJYQV4OF86fyoXzpwLQ1tHF0y8dY3NQJL/e/grrHtsHwLTiBOfOLWdZVTmtRzpZ0txGaUFelPElQ6k4RGIkLyeLs2eXcfbsMv72LafS1eU8f6Dx5BzJ5t2H+dW2/QB8ZeP9TC3K49RTilhQUcT8U4qYP62Y+dOKqJic0OhERkzFIRJjWVnGm6YX86bpxVx53hzcnZeOtXDP/X+koGIezx84Tt2BRtZveYmGltfumFicyOHUaUXMDx4Lgq/JsgIdFiyDUnGIZBAzY1bpJGpOyWHFRfNOrnd3Dh5vpe5AI3UHG6k70MjzrzTy4J8Ocs/j+05ul8jJYu7UQhZUFAcjlNSjamoBiRwdHiwpKg6RCcDMmDY5n2mT87kgmC/pdqy5PSiT1Oik7kAjW/Ye4ZfbXqL7vMfsLGN2eQHTJ+czvSSfaZMTVBSnlismJ5hWnFoXZbl0dTkNLe0caW7nSHMbnV3e43Dp1x86nZeTOiRaH9eNjIpDZIIrKcjlnDllnDOn7HXrT7R18sLBRl4IRig7DzbxckMLm3cf5kBDK22dXW94r/LCPKYVJ6iYnM/0yUGpnFxOPZ9SlBj047D2zi6ONrdztLmNI83tHG5qO7l8pLmNI009lpvbTm7bNczzMl87J+f159q8/uvrS6fh1VZ28AJTCvMoL8yjvCjv5HJRImdcy8jdaTjRwaGmVg43tZ18HAq+1u9rZcWKsf++Kg4R6dOkvGwWzyph8aw33obX3TnS3M4rDS09Hq2vW96xv4GDja30uloL2VnGKUUJKianCub40RZu27UpVQRNqSI43mM+pre8nCzKC/IoLcilvDCPM6ZPprQgl7KCPMoK8ygLlnOy7XWXeulefm1dcAmYk5eHee0yMd3PW9q7OHai/eRrLe2dHG7s4L7dz/adLTuLssJcygsTrxVLYVAsQcGUFeQxpSiP8sIEpZNyyepRoh2dXSeLsrsMjvQogkNNbRxuDAoiKNCOftqyMC+b8sTor3DQFxWHiAybmZ38pXjGjMn9btfR2cWrjW280tDCyw0tHAhK5eWgYF481Myhhi5mWBtlhXlUTSmgrEcplBa8VgTdpTApNzvSj5geeOABll3wZyd/kb/2i731db/YDzW1sfdIM4cb2zje2ncRZhmUFeRRlJ/DsRPtHG1u7/f7lkzKPVlGc6YUcPacUsp7FVHPssrPzaa2tjaUPwMVh4iEJic7i+klqbmQmn62qa2tZcWKN49rrtEwMwoTORQmcqgsLxjSPq0dnRxpan/DR0rdBdPY0kHJpFRZpkqge6SSCAo0l9zs9LnTt4pDRCRkiZxsppdkM70kP+ooYyJ9KkxERGJBxSEiIsOi4hARkWFRcYiIyLCEWhxmttLMnjOzOjO7to/XE2Z2d/D6RjOr6vHadcH658zsHUN9TxERCVdoxWFm2cDNwDuBhcDlZraw12YfBY64+3zgJuBrwb4LgdXAImAlcIuZZQ/xPUVEJERhjjiWAXXuvtPd24C7gFW9tlkF3B4s3wNcaqkze1YBd7l7q7vvAuqC9xvKe4qISIjCPI9jFrC3x/N9wPL+tnH3DjM7BkwJ1j/aa99ZwfJg7wmAma0B1gBUVFSM+AzKxsbG0M6+HGtxygrxyhunrBCvvHHKCvHKG1bWjD0B0N3XAmsBzOzgxRdf/OII32oq8OqYBQtXnLJCvPLGKSvEK2+cskK88o4m65z+XgizOOqByh7Pk8G6vrbZZ2Y5QAlwaJB9B3vPN3D3U4aVvAcze8zdl450//EUp6wQr7xxygrxyhunrBCvvGFlDXOOYzOwwMzmmlkeqcnu9b22WQ9cFSy/H9jg7h6sXx0cdTUXWABsGuJ7iohIiEIbcQRzFtcAvwayge+5+3Yz+xLwmLuvB24FfmhmdcBhUkVAsN064BmgA7ja3TsB+nrPsH4GERF5o1DnONz9XuDeXuuu77HcAnygn31vAG4YynuGbO04fq/RilNWiFfeOGWFeOWNU4GJC1sAAAUfSURBVFaIV95Qspr3vsuKiIjIAHTJERERGRYVh4iIDIuKox9xuiaWmVWa2QNm9oyZbTezT0WdaTDBJWSeNLNfRp1lMGZWamb3mNmzZrbDzM6POlN/zOzvg78DT5vZnWaWVncOMrPvmdkBM3u6x7pyM7vfzJ4PvpZFmbFbP1n/Nfh7sM3MfmZmpVFm7KmvvD1e+6yZuZlNHYvvpeLoQwyvidUBfNbdFwLnAVeneV6ATwE7og4xRP8O3OfupwM1pGluM5sF/B2w1N0XkzrycHW0qd7gNlLXn+vpWuB37r4A+F3wPB3cxhuz3g8sdvdq4E/AdeMdagC38ca8mFkl8HZgz1h9IxVH32J1TSx33+/uTwTLx0n9Yps18F7RMbMk8G7gu1FnGYyZlQAXkTp0HHdvc/ej0aYaUA4wKTihtgB4KeI8r+PuD5E69L6nntesux1437iG6kdfWd39N+7eETx9lNRJyGmhnz9bSF1A9nPAmB0JpeLoW1/X2UrbX8Q9BZemPwvYGG2SAX2d1F/krqiDDMFc4CDw/eCjte+aWWHUofri7vXAv5H6n+V+4Ji7/ybaVENS4e77g+WXgYoowwzD3wD/FXWIgZjZKqDe3beO5fuqODKImRUBPwE+7e4NUefpi5m9Bzjg7o9HnWWIcoCzgW+7+1lAE+nzUcrrBHMDq0iV3Uyg0Mw+HG2q4QmuHJH25wiY2RdIfUR8R9RZ+mNmBcDngesH23a4VBx9G8p1ttKKmeWSKo073P2nUecZwIXAZWa2m9RHgJeY2Y+ijTSgfcA+d+8ewd1DqkjS0VuBXe5+0N3bgZ8CF0ScaSheMbMZAMHXAxHnGZCZfQR4D3CFp/eJcKeS+k/E1uDfWxJ4wsymj/aNVRx9i9U1sYJ7mNwK7HD3G6POMxB3v87dk+5eRerPdYO7p+3/it39ZWCvmb0pWHUpqUvhpKM9wHlmVhD8nbiUNJ3I76XnNeuuAn4RYZYBmdlKUh+zXubuzVHnGYi7P+Xu09y9Kvj3tg84O/g7PSoqjj4Ek1/d18TaAaxL82tiXQhcSep/71uCx7uiDpVBPgncYWbbgCXAVyLO06dgVHQP8ATwFKl/32l1eQwzuxN4BHiTme0zs48CXwXeZmbPkxo1fTXKjN36yfotoBi4P/h39n8jDdlDP3nD+V7pPdISEZF0oxGHiIgMi4pDRESGRcUhIiLDouIQEZFhUXGIiMiwqDhExoCZdfY4FHrLWF5R2cyq+rriqUhUQr11rMgEcsLdl0QdQmQ8aMQhEiIz221m/2JmT5nZJjObH6yvMrMNwX0dfmdms4P1FcF9HrYGj+5LhmSb2X8E99r4jZlNiuyHkglPxSEyNib1+qjqgz1eO+buZ5I66/jrwbpvArcH93W4A/hGsP4bwIPuXkPqmljdVyxYANzs7ouAo8B/C/nnEemXzhwXGQNm1ujuRX2s3w1c4u47gwtRvuzuU8zsVWCGu7cH6/e7+1QzOwgk3b21x3tUAfcHNzrCzP4HkOvu/xT+TybyRhpxiITP+1kejtYey51oflIipOIQCd8He3x9JFh+mNdu63oF8Ptg+XfAJ+DkfdlLxiukyFDpfy0iY2OSmW3p8fw+d+8+JLcsuLJuK3B5sO6TpO4q+A+k7jD418H6TwFrgyubdpIqkf2IpBHNcYiEKJjjWOrur0adRWSs6KMqEREZFo04RERkWDTiEBGRYVFxiIjIsKg4RERkWFQcIiIyLCoOEREZlv8P2zehit6VURAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "END OF EPOCH 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 77/405 [00:10<00:42,  7.71it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t55FUkOGh9pT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}